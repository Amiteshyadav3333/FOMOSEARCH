#!/usr/bin/env python3
"""
FOMO Search Engine - Massive Web Crawler
Run this to populate your search database with thousands of websites
"""

from crawler import MassiveCrawler
import sys
import time

def main():
    print("ğŸ•·ï¸ FOMO SEARCH ENGINE - MASSIVE WEB CRAWLER")
    print("=" * 60)
    print("This will crawl hundreds of popular websites and add them to your search database!")
    print()
    
    # Ask user for crawling scale
    print("Choose your crawling scale:")
    print("1. ğŸƒ Quick Start (1,000 pages) - 5-10 minutes")
    print("2. ğŸš€ Medium Scale (10,000 pages) - 30-60 minutes") 
    print("3. ğŸ”¥ Large Scale (50,000 pages) - 2-4 hours")
    print("4. ğŸŒŸ INSANE Scale (100,000+ pages) - 4-8 hours")
    print()
    
    choice = input("Enter your choice (1-4): ").strip()
    
    if choice == "1":
        crawler = MassiveCrawler(max_workers=10, max_pages=1000, delay_range=(0.5, 1.5))
        print("\nğŸƒ Starting Quick Crawl...")
    elif choice == "2":
        crawler = MassiveCrawler(max_workers=20, max_pages=10000, delay_range=(0.3, 1.0))
        print("\nğŸš€ Starting Medium Scale Crawl...")
    elif choice == "3":
        crawler = MassiveCrawler(max_workers=30, max_pages=50000, delay_range=(0.2, 0.8))
        print("\nğŸ”¥ Starting Large Scale Crawl...")
    elif choice == "4":
        crawler = MassiveCrawler(max_workers=50, max_pages=100000, delay_range=(0.1, 0.5))
        print("\nğŸŒŸ Starting INSANE Scale Crawl...")
    else:
        print("âŒ Invalid choice!")
        return
    
    print(f"ğŸ¯ Target: {crawler.max_pages:,} pages")
    print(f"âš¡ Workers: {crawler.max_workers}")
    print(f"ğŸŒ Seed URLs: {len(crawler.get_seed_urls())}")
    print()
    
    # Confirm before starting
    confirm = input("Ready to start crawling? This will take time and bandwidth. (y/N): ").strip().lower()
    if confirm != 'y':
        print("âŒ Crawling cancelled.")
        return
    
    print("\nğŸš€ STARTING MASSIVE CRAWL...")
    print("=" * 60)
    
    try:
        # Run the crawler
        start_time = time.time()
        pages_crawled = crawler.run_massive_crawl()
        total_time = time.time() - start_time
        
        print("\n" + "=" * 60)
        print("ğŸ‰ CRAWLING COMPLETED SUCCESSFULLY!")
        print(f"âœ… Total pages crawled: {pages_crawled:,}")
        print(f"â±ï¸ Total time: {total_time/60:.1f} minutes")
        print(f"âš¡ Average speed: {pages_crawled/total_time:.1f} pages/second")
        print()
        print("ğŸ” Your FOMO Search Engine is now ready with massive content!")
        print("ğŸŒ You can now search for:")
        print("   â€¢ Programming tutorials and documentation")
        print("   â€¢ News articles and blog posts") 
        print("   â€¢ Educational content and courses")
        print("   â€¢ Technical references and wikis")
        print("   â€¢ And much more!")
        print()
        print("ğŸš€ Start your search engine with: python3 app.py")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Crawling interrupted by user")
        print(f"âœ… Partial crawl completed: {crawler.stats['crawled']:,} pages")
    except Exception as e:
        print(f"\nâŒ Crawling error: {e}")
        print(f"âœ… Partial crawl completed: {crawler.stats['crawled']:,} pages")

if __name__ == "__main__":
    main()